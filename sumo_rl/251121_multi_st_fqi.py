# -*- coding: utf-8 -*-
"""251121_multi ST-FQI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qjV3Nb1B_9p1YMbojgqJOavUiZ-494Se
"""

from google.colab import drive
drive.mount('/content/drive')

# ============================================
# Multi-agent Offline ST-FQI for Seoul Traffic
# (3 intersections near Yonsei, queue-based sim)
# ============================================

import numpy as np
import pandas as pd
from dataclasses import dataclass
from typing import Dict, List, Tuple, Callable

from sklearn.ensemble import ExtraTreesRegressor, RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import log_loss

import matplotlib.pyplot as plt

# -------------------------------------------------
# 0. Global configuration
# -------------------------------------------------

# Intersections (point_id in volume CSV)
INTERSECTIONS = ["D-16", "A-01", "D-17"]

# Simulation parameters
CYCLE_LENGTH = 60.0      # seconds per signal cycle
SERVICE_RATE = 0.5       # vehicles per second when green (capacity parameter)
GREEN_MIN = 0.2          # minimum green split ratio
GREEN_MAX = 0.8          # maximum green split ratio
GREEN_ACTIONS = np.array([0.3, 0.4, 0.5, 0.6, 0.7])  # discrete green splits

# RL parameters
DISCOUNT_GAMMA = 0.95

# OES weights (Team10 definition)
ALPHA_OES = 1.0
BETA_OES = 0.015
GAMMA_OES = 1.0

# -------------------------------------------------
# 1. Data loading & preprocessing
# -------------------------------------------------

def load_volume_data(
    volume_csv_path: str,
    intersections: List[str] = INTERSECTIONS
) -> pd.DataFrame:
    """
    Load and preprocess hourly traffic volume data.
    Expected columns in CSV (based on Team10 preprocessing example):
      - timestamp (or a datetime index)
      - point_id
      - traffic_volume
      - day_of_week (optional)
    We pivot to get a time-indexed DataFrame with columns = intersections.
    """
    df = pd.read_csv(volume_csv_path)

    # Try to infer timestamp column name
    ts_col_candidates = [c for c in df.columns if "time" in c.lower() or "datetime" in c.lower()]
    assert len(ts_col_candidates) >= 1, "Timestamp column not found in volume csv."
    ts_col = ts_col_candidates[0]

    # Ensure datetime
    df[ts_col] = pd.to_datetime(df[ts_col])

    # Filter selected intersections
    df = df[df["point_id"].isin(intersections)].copy()

    # Pivot: index=time, columns=point_id, values=traffic_volume
    pivot = df.pivot_table(
        index=ts_col,
        columns="point_id",
        values="traffic_volume"
    ).sort_index()

    # Forward-fill / back-fill potential missing values
    pivot = pivot.reindex(
        pd.date_range(start=pivot.index.min(),
                      end=pivot.index.max(),
                      freq="H")
    )
    pivot = pivot.interpolate().ffill().bfill()

    # Keep columns in fixed order
    pivot = pivot[intersections]

    return pivot


def volume_to_arrival_rate(volume: float) -> float:
    """
    Convert hourly traffic volume (veh/hour) to arrival rate per second.
    """
    return volume / 3600.0

# -------------------------------------------------
# 2. Queue-based multi-agent environment (no SUMO)
# -------------------------------------------------

@dataclass
class MultiIntersectionState:
    queues: np.ndarray      # shape (n_intersections,)
    arrivals: np.ndarray    # shape (n_intersections,)
    neighbor_pressures: np.ndarray  # shape (n_intersections,)


class QueueEnv:
    """
    Macroscopic queue dynamics:
      q_i(t+1) = max{ 0, q_i(t) + Î»_i(t) * C - s * g_i(t) * C }
    where:
       - Î»_i(t) : arrival rate (veh/sec)
       - C      : cycle length (sec)
       - s      : service rate (veh/sec) at green
       - g_i(t) : green split ratio (0~1)
    Reward: negative of queue-area proxy.
    """

    def __init__(
        self,
        arrival_df: pd.DataFrame,
        cycle_length: float = CYCLE_LENGTH,
        service_rate: float = SERVICE_RATE,
        green_min: float = GREEN_MIN,
        green_max: float = GREEN_MAX,
        green_actions: np.ndarray = GREEN_ACTIONS
    ):
        self.arrival_df = arrival_df
        self.timestamps = arrival_df.index
        self.n_steps = len(self.timestamps)
        self.intersections = list(arrival_df.columns)
        self.n_agents = len(self.intersections)

        self.C = cycle_length
        self.s = service_rate
        self.green_min = green_min
        self.green_max = green_max
        self.green_actions = green_actions

        self.t = 0
        self.queues = np.zeros(self.n_agents, dtype=float)

    def reset(self, noise_std: float = 0.05) -> MultiIntersectionState:
        self.t = 0
        self.queues = np.zeros(self.n_agents, dtype=float)
        return self._build_state(noise_std=noise_std)

    def _get_arrivals(self, t: int, noise_std: float = 0.05) -> np.ndarray:
        # í˜¹ì‹œë¼ë„ tê°€ ë²”ìœ„ë¥¼ ë„˜ì–´ê°€ë©´ ë§ˆì§€ë§‰ ì¸ë±ìŠ¤ë¡œ í´ëž¨í”„
        t = max(0, min(t, self.n_steps - 1))

        base_volumes = self.arrival_df.iloc[t].values  # veh/hour
        lam = np.array([volume_to_arrival_rate(v) for v in base_volumes])  # veh/sec
        # Add mild noise to mimic stochastic arrivals
        if noise_std > 0:
            lam = np.maximum(
                lam + np.random.randn(*lam.shape) * lam * noise_std,
                0.0
            )
        return lam

    def _build_state(self, noise_std: float = 0.05) -> MultiIntersectionState:
        lam = self._get_arrivals(self.t, noise_std=noise_std)
        neighbor_pressures = []
        for i in range(self.n_agents):
            # Sum of queues excluding self = compressed neighbor message
            neighbor_pressures.append(np.sum(self.queues) - self.queues[i])
        neighbor_pressures = np.array(neighbor_pressures)
        return MultiIntersectionState(
            queues=self.queues.copy(),
            arrivals=lam,
            neighbor_pressures=neighbor_pressures
        )

    def step(
        self,
        green_actions_idx: np.ndarray,
        noise_std: float = 0.05
    ) -> Tuple[MultiIntersectionState, np.ndarray, Dict]:
        """
        green_actions_idx: integer indices into self.green_actions for each agent.
        Returns:
          next_state, rewards (array shape (n_agents,)), info dict including metrics.
        """
        assert len(green_actions_idx) == self.n_agents

        # í˜„ìž¬ ì‹œê° tì—ì„œ ë„ì°©ë¥  ê³„ì‚°
        lam = self._get_arrivals(self.t, noise_std=noise_std)

        # ì•¡ì…˜ ì¸ë±ìŠ¤ë¥¼ green ë¹„ìœ¨ë¡œ ë³€í™˜
        g = self.green_actions[green_actions_idx]
        g = np.clip(g, self.green_min, self.green_max)

        capacity = self.s * g * self.C  # veh per cycle that can be served
        queue_before = self.queues.copy()

        arrivals_veh = lam * self.C

        served = np.minimum(queue_before + arrivals_veh, capacity)
        queue_after = np.maximum(queue_before + arrivals_veh - served, 0.0)

        # New waiters (vehicles that arrived but could not be served this cycle)
        backlog_old = np.maximum(queue_before - capacity, 0.0)
        new_waiters = np.maximum(queue_after - backlog_old, 0.0)

        # Reward: negative queue-area proxy (approx. delay)
        rewards = -queue_before * self.C

        # Aggregate metrics to use later: throughput, wait time, stops
        throughput = served
        wait_time = queue_before * self.C   # veh * sec per cycle
        stops = new_waiters                 # count vehicles that had to stop

        # ìƒíƒœ / time ì—…ë°ì´íŠ¸
        self.queues = queue_after
        self.t += 1
        done = self.t >= min(self.n_steps, getattr(self, "max_steps", self.n_steps))

        # ðŸ”´ í•µì‹¬: doneì´ë©´ arrival_dfë¥¼ ë” ì´ìƒ ì•ˆ ë³¸ë‹¤
        if done:
            # í„°ë¯¸ë„ ìƒíƒœ: íëŠ” ìœ ì§€, ë„ì°©ë¥ ì€ 0ìœ¼ë¡œ
            lam_next = np.zeros_like(lam)
            neighbor_pressures = []
            for i in range(self.n_agents):
                neighbor_pressures.append(np.sum(self.queues) - self.queues[i])
            neighbor_pressures = np.array(neighbor_pressures)
            next_state = MultiIntersectionState(
                queues=self.queues.copy(),
                arrivals=lam_next,
                neighbor_pressures=neighbor_pressures
            )
        else:
            # ì•„ì§ ë ì•„ë‹ˆë©´ ê¸°ì¡´ ë°©ì‹ëŒ€ë¡œ ë‹¤ìŒ ìƒíƒœ êµ¬ì„±
            next_state = self._build_state(noise_std=noise_std)

        info = {
            "throughput": throughput,
            "wait_time": wait_time,
            "stops": stops,
            "queue_before": queue_before,
            "queue_after": queue_after,
        }
        return next_state, rewards, done, info

# -------------------------------------------------
# 3. Baseline controllers
# -------------------------------------------------

def compute_fixed_green_splits(env: QueueEnv) -> np.ndarray:
    """
    Webster-style fixed-time split based on average demand.
    """
    avg_volume = env.arrival_df.mean(axis=0).values  # veh/hour per intersection
    avg_lambda = np.array([volume_to_arrival_rate(v) for v in avg_volume])  # veh/sec
    # We want s * g * C â‰ˆ Î» * C  -> g â‰ˆ Î» / s
    g = np.clip(avg_lambda / env.s, env.green_min, env.green_max)
    # Map to nearest discrete action
    idxs = np.argmin(np.abs(env.green_actions[None, :] - g[:, None]), axis=1)
    return idxs.astype(int)


def fixed_time_policy(env: QueueEnv) -> Callable[[MultiIntersectionState], np.ndarray]:
    fixed_idxs = compute_fixed_green_splits(env)

    def policy_fn(state: MultiIntersectionState) -> np.ndarray:
        return fixed_idxs

    return policy_fn


def responsive_policy(env: QueueEnv, smoothing: float = 0.3) -> Callable[[MultiIntersectionState], np.ndarray]:
    """
    SCOOT-like responsive baseline:
    At each cycle, update smoothed arrival estimate and allocate green proportionally.
    """
    lam_smooth = np.zeros(env.n_agents, dtype=float)

    def policy_fn(state: MultiIntersectionState) -> np.ndarray:
        nonlocal lam_smooth
        lam_obs = state.arrivals
        lam_smooth = (1 - smoothing) * lam_smooth + smoothing * lam_obs
        g = np.clip(lam_smooth / env.s, env.green_min, env.green_max)
        idxs = np.argmin(np.abs(env.green_actions[None, :] - g[:, None]), axis=1)
        return idxs.astype(int)

    return policy_fn

# -------------------------------------------------
# 4. Q-learning baseline (tabular, per agent)
# -------------------------------------------------

class TabularQLearningController:
    def __init__(
        self,
        env: QueueEnv,
        n_queue_bins: int = 10,
        n_arrival_bins: int = 5,
        alpha: float = 0.1,
        gamma: float = DISCOUNT_GAMMA,
        eps_start: float = 1.0,
        eps_end: float = 0.05,
        eps_decay_steps: int = 5000
    ):
        self.env = env
        self.n_agents = env.n_agents
        self.n_actions = len(env.green_actions)

        self.n_queue_bins = n_queue_bins
        self.n_arrival_bins = n_arrival_bins

        self.alpha = alpha
        self.gamma = gamma
        self.eps_start = eps_start
        self.eps_end = eps_end
        self.eps_decay_steps = eps_decay_steps
        self.total_steps = 0

        n_states = n_queue_bins * n_arrival_bins
        # One Q-table per agent
        self.Q = np.zeros((self.n_agents, n_states, self.n_actions), dtype=float)

    def _discretize_state_one(self, q: float, lam: float) -> int:
        q_max = 80.0     # vehicles
        lam_max = 0.8    # veh/sec (~2880 veh/h)

        q_bin = int(np.clip(q / q_max * self.n_queue_bins, 0, self.n_queue_bins - 1))
        lam_bin = int(np.clip(lam / lam_max * self.n_arrival_bins, 0, self.n_arrival_bins - 1))
        return q_bin * self.n_arrival_bins + lam_bin

    def _discretize_state(self, state: MultiIntersectionState) -> np.ndarray:
        idxs = []
        for i in range(self.n_agents):
            idxs.append(self._discretize_state_one(state.queues[i], state.arrivals[i]))
        return np.array(idxs, dtype=int)

    def _epsilon(self) -> float:
        if self.total_steps >= self.eps_decay_steps:
            return self.eps_end
        frac = self.total_steps / self.eps_decay_steps
        return self.eps_start + frac * (self.eps_end - self.eps_start)

    def train(self, n_episodes: int = 150, noise_std: float = 0.05):
        for ep in range(n_episodes):
            state = self.env.reset(noise_std=noise_std)
            done = False
            while not done:
                self.total_steps += 1
                s_idx = self._discretize_state(state)
                eps = self._epsilon()

                actions = np.zeros(self.n_agents, dtype=int)
                for i in range(self.n_agents):
                    if np.random.rand() < eps:
                        actions[i] = np.random.randint(self.n_actions)
                    else:
                        actions[i] = np.argmax(self.Q[i, s_idx[i]])

                next_state, rewards, done, _ = self.env.step(actions, noise_std=noise_std)
                s_next_idx = self._discretize_state(next_state)

                for i in range(self.n_agents):
                    a = actions[i]
                    target = rewards[i] + self.gamma * np.max(self.Q[i, s_next_idx[i]])
                    self.Q[i, s_idx[i], a] += self.alpha * (target - self.Q[i, s_idx[i], a])

                state = next_state

    def greedy_policy(self) -> Callable[[MultiIntersectionState], np.ndarray]:
        def policy_fn(state: MultiIntersectionState) -> np.ndarray:
            s_idx = self._discretize_state(state)
            actions = np.zeros(self.n_agents, dtype=int)
            for i in range(self.n_agents):
                actions[i] = int(np.argmax(self.Q[i, s_idx[i]]))
            return actions
        return policy_fn

# -------------------------------------------------
# 5. Offline dataset generation with reactive behavior policy
# -------------------------------------------------

def generate_offline_dataset(
    env: QueueEnv,
    behavior_policy: Callable[[MultiIntersectionState], np.ndarray],
    n_rollouts: int = 50,
    noise_std: float = 0.05
) -> Dict[str, pd.DataFrame]:
    """
    Generate offline dataset Di for each agent i with tuples (s, a, r, s_next).
    Returns:
      dict(agent_id -> DataFrame)
    """
    n_agents = env.n_agents
    agent_ids = env.intersections

    data_per_agent = {aid: [] for aid in agent_ids}

    for k in range(n_rollouts):
        state = env.reset(noise_std=noise_std)
        done = False
        while not done:
            actions = behavior_policy(state)
            next_state, rewards, done, info = env.step(actions, noise_std=noise_std)

            for i, aid in enumerate(agent_ids):
                s_vec = np.array([
                    state.queues[i],
                    state.arrivals[i],
                    state.neighbor_pressures[i]
                ])
                s_next_vec = np.array([
                    next_state.queues[i],
                    next_state.arrivals[i],
                    next_state.neighbor_pressures[i]
                ])
                data_per_agent[aid].append({
                    "q": s_vec[0],
                    "lam": s_vec[1],
                    "neigh": s_vec[2],
                    "action_idx": int(actions[i]),
                    "reward": float(rewards[i]),
                    "q_next": s_next_vec[0],
                    "lam_next": s_next_vec[1],
                    "neigh_next": s_next_vec[2],
                })

            state = next_state

    # Convert to DataFrame
    for aid in agent_ids:
        data_per_agent[aid] = pd.DataFrame(data_per_agent[aid])

    return data_per_agent

# -------------------------------------------------
# 6. ST-FQI implementation (per agent, CTDE)
# -------------------------------------------------

class STFQI_Agent:
    def __init__(
        self,
        tau_support: float = 0.05,
        gamma: float = DISCOUNT_GAMMA,
        n_iters: int = 15
    ):
        self.tau = tau_support
        self.gamma = gamma
        self.n_iters = n_iters

        self.behavior_clf = RandomForestClassifier(
            n_estimators=60,
            max_depth=10,
            min_samples_leaf=10,
            n_jobs=-1
        )
        self.q_reg = ExtraTreesRegressor(
            n_estimators=60,
            max_depth=10,
            min_samples_leaf=10,
            n_jobs=-1
        )
        self.is_fitted = False

    @staticmethod
    def _build_features_state(s: np.ndarray) -> np.ndarray:
        """
        s: (N, 3) = [q, lam, neigh]
        """
        return s

    @staticmethod
    def _build_features_state_action(s: np.ndarray, a_idx: np.ndarray, n_actions: int) -> np.ndarray:
        """
        Concatenate state with one-hot action.
        """
        N = s.shape[0]
        a_onehot = np.zeros((N, n_actions), dtype=float)
        a_onehot[np.arange(N), a_idx] = 1.0
        return np.concatenate([s, a_onehot], axis=1)

    def fit_behavior_model(self, s_arr: np.ndarray, a_arr: np.ndarray):
        X = self._build_features_state(s_arr)
        y = a_arr
        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=True)
        self.behavior_clf.fit(X_train, y_train)
        y_val_proba = self.behavior_clf.predict_proba(X_val)
        # Compute a simple calibration metric (log loss) for monitoring
        ll = log_loss(y_val, y_val_proba)
        print(f"[ST-FQI] Behavior classifier log-loss: {ll:.3f}")

    def _support_actions(self, s: np.ndarray) -> np.ndarray:
        """
        s: (N, 3)
        Returns:
          support_mask: (N, n_actions) Boolean array whether action is in support.
        """
        proba = self.behavior_clf.predict_proba(self._build_features_state(s))
        # RandomForestClassifier returns shape (N, n_classes) in order of classes_
        # We assume actions are encoded 0..n_actions-1, so create full matrix:
        n_actions = proba.shape[1]
        support_mask = proba >= self.tau
        return support_mask, proba

    def fit_fqi(self, dataset: pd.DataFrame, n_actions: int):
        """
        dataset columns:
          q, lam, neigh, action_idx, reward, q_next, lam_next, neigh_next
        """
        # Prepare arrays
        s = dataset[["q", "lam", "neigh"]].values.astype(float)
        a = dataset["action_idx"].values.astype(int)
        r = dataset["reward"].values.astype(float)
        s_next = dataset[["q_next", "lam_next", "neigh_next"]].values.astype(float)

        # 1) fit behavior model
        self.fit_behavior_model(s, a)

        # 2) initialize Q with zeros (via supervised regression on r only or small rollout)
        N = s.shape[0]
        X_sa = self._build_features_state_action(s, a, n_actions)
        y = r.copy()  # first iteration: 1-step reward only
        self.q_reg.fit(X_sa, y)

        # 3) FQI iterations with support-aware targets
        for it in range(self.n_iters):
            # Predict current Q for all actions at s_next
            support_mask, proba = self._support_actions(s_next)
            # For each s_next, choose best supported action
            n_samples = s_next.shape[0]
            target = np.zeros(n_samples, dtype=float)

            # Precompute Q(s_next, a) for all actions by stacking
            all_q_vals = []
            for a_idx in range(n_actions):
                a_vec = np.full(n_samples, a_idx, dtype=int)
                X_next = self._build_features_state_action(s_next, a_vec, n_actions)
                q_vals = self.q_reg.predict(X_next)
                all_q_vals.append(q_vals)
            all_q_vals = np.stack(all_q_vals, axis=1)  # (N, n_actions)

            for i in range(n_samples):
                supported = np.where(support_mask[i])[0]
                if len(supported) > 0:
                    best_idx = supported[np.argmax(all_q_vals[i, supported])]
                else:
                    # Fallback to behavior argmax
                    best_idx = int(np.argmax(proba[i]))
                target[i] = r[i] + self.gamma * all_q_vals[i, best_idx]

            # Refit regressor on (s,a) -> target
            X_sa = self._build_features_state_action(s, a, n_actions)
            self.q_reg.fit(X_sa, target)
            print(f"[ST-FQI] Iter {it+1}/{self.n_iters} done.")

        self.is_fitted = True

    def act_greedy(self, state: MultiIntersectionState, n_actions: int) -> int:
        assert self.is_fitted
        s = np.array([[state.queues, state.arrivals, state.neighbor_pressures]])  # not used; per-agent version below
        raise NotImplementedError("Use per-agent wrapper.")


class MultiAgentSTFQIController:
    def __init__(self, env: QueueEnv, tau_support: float = 0.05, n_iters: int = 15):
        self.env = env
        self.n_agents = env.n_agents
        self.n_actions = len(env.green_actions)
        self.agents = [
            STFQI_Agent(tau_support=tau_support, gamma=DISCOUNT_GAMMA, n_iters=n_iters)
            for _ in range(self.n_agents)
        ]

    def train(self, datasets: Dict[str, pd.DataFrame]):
        for i, aid in enumerate(self.env.intersections):
            print(f"=== Training ST-FQI agent for intersection {aid} ===")
            self.agents[i].fit_fqi(datasets[aid], n_actions=self.n_actions)

    def policy(self) -> Callable[[MultiIntersectionState], np.ndarray]:
        def policy_fn(state: MultiIntersectionState) -> np.ndarray:
            actions = np.zeros(self.n_agents, dtype=int)
            for i in range(self.n_agents):
                s_vec = np.array([[state.queues[i],
                                   state.arrivals[i],
                                   state.neighbor_pressures[i]]])
                # build candidate features for all actions
                all_q = []
                # support mask & proba
                support_mask, proba = self.agents[i]._support_actions(s_vec)
                support_mask = support_mask[0]
                proba = proba[0]
                for a_idx in range(self.n_actions):
                    sa_feat = STFQI_Agent._build_features_state_action(
                        s_vec, np.array([a_idx]), self.n_actions
                    )
                    q_val = self.agents[i].q_reg.predict(sa_feat)[0]
                    all_q.append(q_val)
                all_q = np.array(all_q)

                supported = np.where(support_mask)[0]
                if len(supported) > 0:
                    best_supported = supported[np.argmax(all_q[supported])]
                    actions[i] = int(best_supported)
                else:
                    # rare: fallback to behavior argmax
                    actions[i] = int(np.argmax(proba))
            return actions
        return policy_fn

# -------------------------------------------------
# 7. Evaluation utilities (AWT, TP, ANS, OES)
# -------------------------------------------------

@dataclass
class EvalResult:
    algo_name: str
    awt: float
    tp: float
    ans: float
    oes: float


def evaluate_policy(
    env: QueueEnv,
    policy_fn: Callable[[MultiIntersectionState], np.ndarray],
    n_episodes: int = 10,
    noise_std: float = 0.05
) -> EvalResult:
    total_wait = 0.0
    total_tp = 0.0
    total_stops = 0.0
    total_departures = 0.0

    for ep in range(n_episodes):
        state = env.reset(noise_std=noise_std)
        done = False
        while not done:
            actions = policy_fn(state)
            next_state, rewards, done, info = env.step(actions, noise_std=noise_std)

            total_wait += np.sum(info["wait_time"])
            total_tp += np.sum(info["throughput"])
            total_stops += np.sum(info["stops"])
            total_departures += np.sum(info["throughput"])

            state = next_state

    if total_departures > 0:
        awt = total_wait / total_departures  # sec per vehicle
        ans = total_stops / total_departures
    else:
        awt = np.inf
        ans = np.inf

    tp = total_tp / n_episodes  # veh per episode

    oes = -ALPHA_OES * awt + BETA_OES * tp - GAMMA_OES * ans

    return EvalResult(algo_name="", awt=awt, tp=tp, ans=ans, oes=oes)

# -------------------------------------------------
# 8. Support / extrapolation diagnostics
# -------------------------------------------------

def diagnose_support_coverage(
    stfqi_controller: MultiAgentSTFQIController,
    env: QueueEnv,
    naive_policy: Callable[[MultiIntersectionState], np.ndarray],
    stfqi_policy: Callable[[MultiIntersectionState], np.ndarray],
    n_samples: int = 3000,
    noise_std: float = 0.05
) -> pd.DataFrame:
    """
    Compare Naive FQI-like policy vs ST-FQI in terms of:
      - OOD action rate (behavior prob < tau)
      - Average behavior prob of chosen action
    For simplicity, we approximate 'naive FQI' by using the same Q-regressor but
    IGNORING the support mask in action selection.
    """
    n_agents = env.n_agents
    n_actions = len(env.green_actions)

    rows = []

    # helper to evaluate one policy
    def collect(policy_name: str, use_support_gate: bool):
        state = env.reset(noise_std=noise_std)
        done = False
        while not done and len(rows) < n_samples:
            # We will override actual policy to inspect per-agent behavior model vs chosen action
            if use_support_gate:
                actions = stfqi_policy(state)
            else:
                # naive: argmax Q over all actions without gate
                actions = np.zeros(n_agents, dtype=int)
                for i in range(n_agents):
                    s_vec = np.array([[state.queues[i],
                                       state.arrivals[i],
                                       state.neighbor_pressures[i]]])
                    all_q = []
                    for a_idx in range(n_actions):
                        sa_feat = STFQI_Agent._build_features_state_action(
                            s_vec, np.array([a_idx]), n_actions
                        )
                        q_val = stfqi_controller.agents[i].q_reg.predict(sa_feat)[0]
                        all_q.append(q_val)
                    all_q = np.array(all_q)
                    actions[i] = int(np.argmax(all_q))

            # compute behavior prob & support
            for i in range(n_agents):
                s_vec = np.array([[state.queues[i],
                                   state.arrivals[i],
                                   state.neighbor_pressures[i]]])
                support_mask, proba = stfqi_controller.agents[i]._support_actions(s_vec)
                support_mask = support_mask[0]
                proba = proba[0]
                a = actions[i]
                behavior_p = proba[a]
                is_ood = behavior_p < stfqi_controller.agents[i].tau
                rows.append({
                    "policy": policy_name,
                    "agent": env.intersections[i],
                    "behavior_prob": behavior_p,
                    "is_ood": int(is_ood),
                })

            next_state, rewards, done, info = env.step(actions, noise_std=noise_std)
            state = next_state

    collect("Naive FQI", use_support_gate=False)
    collect("ST-FQI", use_support_gate=True)

    df = pd.DataFrame(rows)
    summary = df.groupby("policy").agg(
        avg_behavior_prob=("behavior_prob", "mean"),
        ood_rate=("is_ood", "mean"),
        n_samples=("is_ood", "count")
    ).reset_index()
    print(summary)
    return summary

# -------------------------------------------------
# 9. Plotting helpers
# -------------------------------------------------

def plot_results(results: List[EvalResult]):
    algos = [r.algo_name for r in results]
    awt = np.array([r.awt for r in results])
    tp = np.array([r.tp for r in results])
    ans = np.array([r.ans for r in results])
    oes = np.array([r.oes for r in results])

    # AWT
    plt.figure()
    plt.bar(algos, awt)
    plt.ylabel("AWT (sec/veh)")
    plt.title("Average Waiting Time by policy")
    plt.grid(axis="y", linestyle="--", alpha=0.4)

    # Throughput
    plt.figure()
    plt.bar(algos, tp)
    plt.ylabel("Throughput (veh/episode)")
    plt.title("Throughput by policy")
    plt.grid(axis="y", linestyle="--", alpha=0.4)

    # ANS
    plt.figure()
    plt.bar(algos, ans)
    plt.ylabel("ANS (stops/veh)")
    plt.title("Average Number of Stops by policy")
    plt.grid(axis="y", linestyle="--", alpha=0.4)

    # OES
    plt.figure()
    plt.bar(algos, oes)
    plt.ylabel("OES")
    plt.title("Overall Evaluation Score (OES)")
    plt.grid(axis="y", linestyle="--", alpha=0.4)


def plot_support_summary(summary_df: pd.DataFrame):
    policies = summary_df["policy"].tolist()
    ood = summary_df["ood_rate"].values
    beh = summary_df["avg_behavior_prob"].values

    x = np.arange(len(policies))
    width = 0.35

    plt.figure()
    plt.bar(x - width/2, ood, width, label="OOD action rate")
    plt.bar(x + width/2, beh, width, label="Avg behavior prob of Ï€(s)")
    plt.xticks(x, policies)
    plt.ylabel("Rate / probability")
    plt.title("Support coverage diagnostics")
    plt.legend()
    plt.grid(axis="y", linestyle="--", alpha=0.4)

# -------------------------------------------------
# 10. Main script (or notebook driver)
# -------------------------------------------------

def main():
    # ===== 1. Load Seoul traffic volume data =====
    volume_csv_path = "/content/drive/MyDrive/Colab Notebooks/seoul_traffic_volume_2025_01_cleaned_en.csv"
    arrival_df = load_volume_data(volume_csv_path, INTERSECTIONS)

    # ===== 2. Build queue environment =====
    env_for_data = QueueEnv(arrival_df)

    # ===== 3. Baseline: fixed-time & responsive =====
    fixed_policy_fn = fixed_time_policy(env_for_data)
    responsive_policy_fn = responsive_policy(env_for_data, smoothing=0.3)

    # ===== 4. Q-learning training =====
    q_env = QueueEnv(arrival_df)  # separate instance
    q_controller = TabularQLearningController(q_env)
    print("Training Q-learning baseline...")
    q_controller.train(n_episodes=1)
    q_policy_fn = q_controller.greedy_policy()

    # ===== 5. Offline dataset with reactive behavior =====
    behavior_env = QueueEnv(arrival_df)
    behavior_policy_fn = responsive_policy(behavior_env, smoothing=0.3)
    print("Generating offline dataset with reactive policy...")
    datasets = generate_offline_dataset(
        behavior_env, behavior_policy_fn, n_rollouts=1
    )

    # ===== 6. Train ST-FQI (per agent, CTDE) =====
    st_env = QueueEnv(arrival_df)
    stfqi_controller = MultiAgentSTFQIController(st_env, tau_support=0.05, n_iters=15)
    stfqi_controller.train(datasets)
    st_policy_fn = stfqi_controller.policy()

    # ===== 7. Evaluate all policies on a fresh env =====
    eval_env = QueueEnv(arrival_df)
    eval_env.max_steps = 100

    res_fixed = evaluate_policy(eval_env, fixed_policy_fn, n_episodes=1)
    res_fixed.algo_name = "Fixed-time"

    res_resp = evaluate_policy(eval_env, responsive_policy_fn, n_episodes=1)
    res_resp.algo_name = "Responsive"

    eval_env_q = QueueEnv(arrival_df)
    eval_env_q.max_steps = 100
    res_q = evaluate_policy(eval_env_q, q_policy_fn, n_episodes=1)
    res_q.algo_name = "Q-learning"

    eval_env_st = QueueEnv(arrival_df)
    eval_env_st.max_steps = 100
    res_st = evaluate_policy(eval_env_st, st_policy_fn, n_episodes=1)
    res_st.algo_name = "ST-FQI"

    results = [res_fixed, res_resp, res_q, res_st]

    for r in results:
        print(f"{r.algo_name}: "
              f"AWT={r.awt:.2f} sec/veh, "
              f"TP={r.tp:.1f} veh/ep, "
              f"ANS={r.ans:.3f} stops/veh, "
              f"OES={r.oes:.2f}")

    # ===== 8. Plot OES-related metrics =====
    plot_results(results)

    # # ===== 9. Support / extrapolation diagnostics =====
    # diag_env = QueueEnv(arrival_df)
    # naive_vs_st_summary = diagnose_support_coverage(
    #     stfqi_controller, diag_env,
    #     naive_policy=None,
    #     stfqi_policy=st_policy_fn,
    #     n_samples=800
    # )
    # plot_support_summary(naive_vs_st_summary)

    plt.show()


if __name__ == "__main__":
    main()

# ===== 9. Support / extrapolation diagnostics =====
    diag_env = QueueEnv(arrival_df)
    naive_vs_st_summary = diagnose_support_coverage(
        stfqi_controller, diag_env,
        naive_policy=None,
        stfqi_policy=st_policy_fn,
        n_samples=800
    )
    plot_support_summary(naive_vs_st_summary)

    plt.show()